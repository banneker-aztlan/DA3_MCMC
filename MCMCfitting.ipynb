{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook follows \"metropolis-hastingsMCMC\" and uses the Metropolis-Hastings MCMC algorithm that we developed last time to explore the parameter space for a dataset where the model has several parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function is a general MCMC walker that can be applied to *any* functional form and set of data, if you provide it with the appropriate function to calculate $\\chi^2$.  Read it over carefully and make sure you understand how it corresponds to the code we wrote yesterday!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mhMCMC(xi, yi, err, chi2func, initparams, stepsize, nsteps):\n",
    "    ''' Performs a Markov Chain Monte Carlo chain using the Metropolis-Hastings algorithm\n",
    "        to explore the parameter space for the given model and data.  Returns the sequence\n",
    "        of steps taken.\n",
    "    \n",
    "        Input\n",
    "        =====\n",
    "        xi : An array giving the x-values of the data\n",
    "        yi : An array giving the y-values of the data\n",
    "        err : An array giving the errors on the data.  \n",
    "              May be 1xN (for y-errors only) or 2xN (for x- and y- errors), \n",
    "              must match what's expected by chi2func.\n",
    "        chi2func : A function which calculates the chi-squared between the data\n",
    "                   and model.  Must take arguments (params, xi, yi, err).\n",
    "        initparams : An array [a0, a1, a2...] giving the initial position in parameter space.\n",
    "        stepsize : An array [da0, da1, da2...] giving the sigma of the normal distributions\n",
    "                   from which the steps are drawn.  Must be the same length as initparams.\n",
    "        nsteps : The number of steps to take.\n",
    "                \n",
    "        Returns\n",
    "        =======\n",
    "        p : An array of positions in parameter space, with dimensions (nsteps+1, nparams)\n",
    "    '''\n",
    "    p = initparams\n",
    "    plist = [p]\n",
    "    \n",
    "    for i in range(0,nsteps):\n",
    "        # Draw a step size for each parameter\n",
    "        dp = np.zeros(len(p))\n",
    "        for j in range(0,len(p)):\n",
    "            if stepsize[j] > 0: dp[j] = np.random.normal(scale=stepsize[j])\n",
    "            \n",
    "        # Check out the chi2 at the new location\n",
    "        if chi2func(p + dp, xi, yi, err) < chi2func(p, xi, yi, err):\n",
    "            # Accept the step if the likelihood is higher\n",
    "            p = p + dp\n",
    "        else:\n",
    "            # Evaluate the likelihood ratio\n",
    "            r = np.math.exp((chi2func(p, xi, yi, err) - chi2func(p + dp, xi, yi, err))/2.)\n",
    "            # Accept the step if the random number drawn is lower than the likelihood ratio\n",
    "            u = np.random.uniform(0., 1.)\n",
    "            if r > u:\n",
    "                p = p + dp\n",
    "    \n",
    "        plist.append(p)\n",
    "\n",
    "    return np.array(plist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "Today we are going to use this MCMC function to fit a set of data with a functional form that is significantly more complicated than the straight line we have worked with before.  The code below reads in and plots the data from a file \"data.npy\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.load(\"data.npy\")\n",
    "xdata = data[0,:]\n",
    "ydata = data[1,:]\n",
    "err = data[2,:]\n",
    "\n",
    "plt.errorbar(xdata, ydata, yerr=yerr, fmt='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, a straight line isn't going to cut it!  This data is actually best described by the superposition of two periodic functions, $y = p_0 \\cos(p_1 x) + p_2 \\sin(p_3 x)$.  Now we have four free parameters instead of two!  \n",
    "\n",
    "(And in reality, you would have six - including a phase angle for each sinusoid - but I've given you $\\phi_1$ = $\\phi_2$ = 0 for free.  Believe it or not, this is actually a plausible functional form for a signal produced by an astrophysical object, and the sort of thing you could encounter in your research.)\n",
    "\n",
    "Below I have written a function that finds the $y$-values given a set of $x$-values and an array of parameters $p = [p_0, p_1, p_2, p_3]$.\n",
    "\n",
    "**To do:** Implement a function which calculates the $\\chi^2$ between this model with a given set of parameters, and a set of $(x,y,\\sigma_y)$ data.  Try out a few sets of parameter values and see how low you can get the $\\chi^2$ with the manual-guess-and-check method.  You can also plot the function with your trial parameters on top of the data for a \"chi-by-eye\".\n",
    "\n",
    "Hints:\n",
    "- It would be best to return a *reduced* $\\chi^2$, which has the number of degrees of freedom (i.e., the number of data points - 1) divided out. \n",
    "- Think about whether there are any regions of parameter space you'd like to exclude.  If there are, returning \"np.infty\" will forbid the MCMC walker from wandering there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def crazyfunction(p, x):\n",
    "    y = p[0]*np.cos(p[1]*x) + p[2]*np.sin(p[3]*x)\n",
    "    return y\n",
    "\n",
    "def chi2crazyfunction(p, x, y, err):\n",
    "    # YOUR CODE HERE\n",
    "    return chi2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "Now, instead of continuing to mess around manually, or calculating the likelihoods throughout the whole 4-dimensional parameter space, we can MCMC walk our way around the space and map the probabilities!\n",
    "\n",
    "**To do:** Define an initial location in parameter space, a typical step size for each parameter, and the total number of steps you'd like to take.  Then, run the MCMC function above using the $\\chi^2$ function you just wrote and the data we have read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FINISH THESE LINES\n",
    "# Initial location\n",
    "pinit = \n",
    "# Step sizes (sigmas of normal distributions)\n",
    "stepsize = \n",
    "# Total number of steps to take\n",
    "nsteps = \n",
    "\n",
    "# Run the MCMC!\n",
    "p ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to always *take a look* at your output so that you can see what happened.  I've set up a plotting routine below that creates four panels stacked on top of one another (one for each parameter).  \n",
    "\n",
    "**To do:** Fill in the lines to plot the evolution of each parameter, and take a look at the output.  Does the MCMC walker move gradually up or down from its initial location, then settle around a constant value?  (This initial period is called *burn-in*.)  As always, take some time to play with the step sizes, initial location, and number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gs1 = gridspec.GridSpec(4,1)\n",
    "gs1.update(left=0.07, right=0.97, top=0.95, bottom=0.1, hspace=0, wspace=0.)\n",
    "ax = plt.subplot(gs1[0,0])\n",
    "# Plot parameter 0\n",
    "ax = plt.subplot(gs1[1,0])\n",
    "# Plot parameter 1\n",
    "ax = plt.subplot(gs1[2,0])\n",
    "# Plot parameter 2\n",
    "ax = plt.subplot(gs1[3,0])\n",
    "# Plot parameter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To do:** What is the median value of each parameter, *after* burn-in?  Plot the function with these median values on top of the data.  How does the fit look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the Metropolis-Hastings algorithm has the property of *ergodicity*, meaning that with a large enough number of steps, the walker will trace out the underlying probability space.  \n",
    "\n",
    "**To do:** Finish the code below to plot the histogram of the locations visited by the walker for each of the four parameters.  Anything interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "gs1 = gridspec.GridSpec(1,4)\n",
    "gs1.update(left=0.07, right=0.97, top=0.95, bottom=0.1, hspace=0, wspace=0.2)\n",
    "for i in range(0,4):\n",
    "    ax = plt.subplot(gs1[0,i])\n",
    "    # Your code here - plot the histogram of the i'th parameter\n",
    "    ax.set_xlabel('p['+str(i)+']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting 2d histograms, you can also see how any two parameters are related.  The code below computes and plots a 2d histogram for p[0] and p[2].  Take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hist, xedges, yedges = np.histogram2d(p[:,0], p[:,2], bins=25)\n",
    "plt.imshow(np.flipud(hist), interpolation='nearest', extent=[yedges[0], yedges[-1], xedges[0], xedges[-1]], aspect='auto')\n",
    "plt.xlabel('p[2]')\n",
    "plt.ylabel('p[0]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus goals**\n",
    "\n",
    "- You may have noticed that some parameters are more well-constrained than others.  To get a better estimate of the harder-to-constrain parameters, you can rerun the MCMC with some parameters fixed in place, i.e. with step size = 0.  Give it a try!\n",
    "\n",
    "- We have not discussed what happens if you have **priors** on the parameter values, i.e. you know that some values are more likely than others.  Let's say we had a prior for p[0] that was a Gaussian centered on centered on 7.  How would you go about implementing this prior probability?  How would the probability space change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
